#!/usr/bin/env python3\n\"\"\"\nComprehensive system testing script for AI Customer Query System\n\"\"\"\n\nimport asyncio\nimport aiohttp\nimport json\nimport time\nfrom typing import List, Dict\nimport random\n\n# Test data\nTEST_QUERIES = [\n    {\"query\": \"My bill is way too high this month\", \"expected_intent\": \"billing\", \"customer_id\": \"test_001\"},\n    {\"query\": \"The app keeps crashing when I try to login\", \"expected_intent\": \"technical\", \"customer_id\": \"test_002\"},\n    {\"query\": \"I'm extremely disappointed with your service\", \"expected_intent\": \"complaint\", \"customer_id\": \"test_003\"},\n    {\"query\": \"Hello, I need some general information\", \"expected_intent\": \"general\", \"customer_id\": \"test_004\"},\n    {\"query\": \"URGENT: My account has been charged incorrectly!\", \"expected_intent\": \"billing\", \"customer_id\": \"test_005\"},\n    {\"query\": \"The system error 500 keeps appearing\", \"expected_intent\": \"technical\", \"customer_id\": \"test_006\"},\n    {\"query\": \"I want to cancel my subscription immediately\", \"expected_intent\": \"cancellation\", \"customer_id\": \"test_007\"},\n    {\"query\": \"Can I upgrade to premium plan?\", \"expected_intent\": \"upgrade\", \"customer_id\": \"test_008\"},\n]\n\nclass SystemTester:\n    def __init__(self, base_url: str = \"http://localhost:8000\"):\n        self.base_url = base_url\n        self.results = []\n        \n    async def test_health_endpoint(self, session: aiohttp.ClientSession) -> Dict:\n        \"\"\"Test health check endpoint\"\"\"\n        print(\"Testing health endpoint...\")\n        \n        try:\n            async with session.get(f\"{self.base_url}/health\") as response:\n                data = await response.json()\n                \n                assert response.status == 200\n                assert \"status\" in data\n                assert \"model_health\" in data\n                assert \"system_health\" in data\n                \n                print(f\"✅ Health check passed: {data['status']}\")\n                return {\"test\": \"health\", \"status\": \"passed\", \"response\": data}\n                \n        except Exception as e:\n            print(f\"❌ Health check failed: {e}\")\n            return {\"test\": \"health\", \"status\": \"failed\", \"error\": str(e)}\n    \n    async def test_query_analysis(self, session: aiohttp.ClientSession, query_data: Dict) -> Dict:\n        \"\"\"Test query analysis endpoint\"\"\"\n        print(f\"Testing query: '{query_data['query'][:50]}...'\")\n        \n        try:\n            payload = {\n                \"query\": query_data[\"query\"],\n                \"customer_id\": query_data[\"customer_id\"],\n                \"channel\": \"api_test\",\n                \"customer_tier\": \"standard\"\n            }\n            \n            start_time = time.time()\n            \n            async with session.post(\n                f\"{self.base_url}/analyze-query\",\n                json=payload\n            ) as response:\n                \n                response_time = (time.time() - start_time) * 1000\n                data = await response.json()\n                \n                assert response.status == 200\n                assert \"intent\" in data\n                assert \"sentiment\" in data\n                assert \"confidence\" in data\n                assert \"priority\" in data\n                assert \"response\" in data\n                \n                # Check if intent matches expected (if provided)\n                intent_correct = True\n                if \"expected_intent\" in query_data:\n                    intent_correct = data[\"intent\"] == query_data[\"expected_intent\"]\n                \n                result = {\n                    \"test\": \"query_analysis\",\n                    \"query\": query_data[\"query\"],\n                    \"expected_intent\": query_data.get(\"expected_intent\"),\n                    \"actual_intent\": data[\"intent\"],\n                    \"intent_correct\": intent_correct,\n                    \"confidence\": data[\"confidence\"],\n                    \"priority\": data[\"priority\"],\n                    \"response_time_ms\": response_time,\n                    \"status\": \"passed\" if intent_correct else \"warning\"\n                }\n                \n                status_emoji = \"✅\" if intent_correct else \"⚠️\"\n                print(f\"{status_emoji} Intent: {data['intent']} (confidence: {data['confidence']:.3f})\")\n                \n                return result\n                \n        except Exception as e:\n            print(f\"❌ Query analysis failed: {e}\")\n            return {\n                \"test\": \"query_analysis\",\n                \"query\": query_data[\"query\"],\n                \"status\": \"failed\",\n                \"error\": str(e)\n            }\n    \n    async def test_metrics_endpoint(self, session: aiohttp.ClientSession) -> Dict:\n        \"\"\"Test metrics endpoint\"\"\"\n        print(\"Testing metrics endpoint...\")\n        \n        try:\n            async with session.get(f\"{self.base_url}/metrics\") as response:\n                data = await response.json()\n                \n                assert response.status == 200\n                assert \"query_counts\" in data\n                assert \"avg_confidence\" in data\n                \n                print(f\"✅ Metrics retrieved: {len(data['query_counts'])} intent categories\")\n                return {\"test\": \"metrics\", \"status\": \"passed\", \"response\": data}\n                \n        except Exception as e:\n            print(f\"❌ Metrics test failed: {e}\")\n            return {\"test\": \"metrics\", \"status\": \"failed\", \"error\": str(e)}\n    \n    async def test_drift_detection(self, session: aiohttp.ClientSession) -> Dict:\n        \"\"\"Test model drift detection\"\"\"\n        print(\"Testing drift detection...\")\n        \n        try:\n            async with session.get(f\"{self.base_url}/model-drift\") as response:\n                data = await response.json()\n                \n                assert response.status == 200\n                assert \"drift_detected\" in data\n                \n                drift_status = \"detected\" if data[\"drift_detected\"] else \"not detected\"\n                print(f\"✅ Drift status: {drift_status}\")\n                \n                return {\"test\": \"drift_detection\", \"status\": \"passed\", \"response\": data}\n                \n        except Exception as e:\n            print(f\"❌ Drift detection test failed: {e}\")\n            return {\"test\": \"drift_detection\", \"status\": \"failed\", \"error\": str(e)}\n    \n    async def load_test(self, session: aiohttp.ClientSession, concurrent_requests: int = 10) -> Dict:\n        \"\"\"Perform load testing\"\"\"\n        print(f\"Performing load test with {concurrent_requests} concurrent requests...\")\n        \n        async def single_request():\n            query_data = random.choice(TEST_QUERIES)\n            payload = {\n                \"query\": query_data[\"query\"],\n                \"customer_id\": f\"load_test_{random.randint(1000, 9999)}\",\n                \"channel\": \"load_test\"\n            }\n            \n            start_time = time.time()\n            try:\n                async with session.post(\n                    f\"{self.base_url}/analyze-query\",\n                    json=payload\n                ) as response:\n                    await response.json()\n                    return time.time() - start_time\n            except Exception:\n                return None\n        \n        start_time = time.time()\n        tasks = [single_request() for _ in range(concurrent_requests)]\n        response_times = await asyncio.gather(*tasks)\n        total_time = time.time() - start_time\n        \n        successful_requests = [rt for rt in response_times if rt is not None]\n        success_rate = len(successful_requests) / len(response_times)\n        avg_response_time = sum(successful_requests) / len(successful_requests) if successful_requests else 0\n        \n        result = {\n            \"test\": \"load_test\",\n            \"concurrent_requests\": concurrent_requests,\n            \"total_time\": total_time,\n            \"success_rate\": success_rate,\n            \"avg_response_time\": avg_response_time,\n            \"requests_per_second\": concurrent_requests / total_time,\n            \"status\": \"passed\" if success_rate > 0.9 else \"warning\"\n        }\n        \n        print(f\"✅ Load test completed:\")\n        print(f\"   Success rate: {success_rate:.1%}\")\n        print(f\"   Avg response time: {avg_response_time:.3f}s\")\n        print(f\"   Requests/sec: {result['requests_per_second']:.1f}\")\n        \n        return result\n    \n    async def run_all_tests(self) -> Dict:\n        \"\"\"Run comprehensive test suite\"\"\"\n        print(\"🚀 Starting comprehensive system tests...\\n\")\n        \n        async with aiohttp.ClientSession() as session:\n            # Basic functionality tests\n            health_result = await self.test_health_endpoint(session)\n            self.results.append(health_result)\n            \n            # Wait a moment for system to be ready\n            await asyncio.sleep(1)\n            \n            # Test all query types\n            for query_data in TEST_QUERIES:\n                result = await self.test_query_analysis(session, query_data)\n                self.results.append(result)\n                await asyncio.sleep(0.5)  # Small delay between requests\n            \n            # Test metrics and monitoring\n            metrics_result = await self.test_metrics_endpoint(session)\n            self.results.append(metrics_result)\n            \n            drift_result = await self.test_drift_detection(session)\n            self.results.append(drift_result)\n            \n            # Load testing\n            load_result = await self.load_test(session, concurrent_requests=5)\n            self.results.append(load_result)\n        \n        return self.generate_report()\n    \n    def generate_report(self) -> Dict:\n        \"\"\"Generate comprehensive test report\"\"\"\n        total_tests = len(self.results)\n        passed_tests = len([r for r in self.results if r.get(\"status\") == \"passed\"])\n        failed_tests = len([r for r in self.results if r.get(\"status\") == \"failed\"])\n        warning_tests = len([r for r in self.results if r.get(\"status\") == \"warning\"])\n        \n        # Calculate intent accuracy\n        query_tests = [r for r in self.results if r.get(\"test\") == \"query_analysis\"]\n        correct_intents = len([r for r in query_tests if r.get(\"intent_correct\", False)])\n        intent_accuracy = correct_intents / len(query_tests) if query_tests else 0\n        \n        # Calculate average response time\n        response_times = [r.get(\"response_time_ms\", 0) for r in query_tests if \"response_time_ms\" in r]\n        avg_response_time = sum(response_times) / len(response_times) if response_times else 0\n        \n        report = {\n            \"summary\": {\n                \"total_tests\": total_tests,\n                \"passed\": passed_tests,\n                \"failed\": failed_tests,\n                \"warnings\": warning_tests,\n                \"success_rate\": passed_tests / total_tests if total_tests > 0 else 0\n            },\n            \"performance\": {\n                \"intent_accuracy\": intent_accuracy,\n                \"avg_response_time_ms\": avg_response_time\n            },\n            \"detailed_results\": self.results\n        }\n        \n        print(\"\\n\" + \"=\"*50)\n        print(\"📊 TEST REPORT SUMMARY\")\n        print(\"=\"*50)\n        print(f\"Total Tests: {total_tests}\")\n        print(f\"✅ Passed: {passed_tests}\")\n        print(f\"⚠️  Warnings: {warning_tests}\")\n        print(f\"❌ Failed: {failed_tests}\")\n        print(f\"Success Rate: {report['summary']['success_rate']:.1%}\")\n        print(f\"Intent Accuracy: {intent_accuracy:.1%}\")\n        print(f\"Avg Response Time: {avg_response_time:.1f}ms\")\n        print(\"=\"*50)\n        \n        return report\n\nasync def main():\n    \"\"\"Main test execution\"\"\"\n    tester = SystemTester()\n    report = await tester.run_all_tests()\n    \n    # Save report to file\n    with open(\"test_report.json\", \"w\") as f:\n        json.dump(report, f, indent=2)\n    \n    print(f\"\\n📄 Detailed report saved to: test_report.json\")\n    \n    # Exit with appropriate code\n    if report[\"summary\"][\"failed\"] > 0:\n        exit(1)\n    elif report[\"summary\"][\"warnings\"] > 0:\n        exit(2)\n    else:\n        exit(0)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())