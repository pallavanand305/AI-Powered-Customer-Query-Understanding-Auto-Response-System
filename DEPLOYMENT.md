# üöÄ Deployment Guide\n\n## Local Development Deployment\n\n### Prerequisites\n- Python 3.9+\n- Docker & Docker Compose\n- Git\n\n### Step 1: Environment Setup\n```bash\n# Clone repository\ngit clone <your-repo-url>\ncd AI-Powered-Customer-Query-Understanding-Auto-Response-System\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n### Step 2: Configuration\n```bash\n# Copy environment template\ncp .env.example .env\n\n# Edit configuration (optional)\n# MLFLOW_TRACKING_URI=http://localhost:5000\n# AWS_REGION=us-east-1\n# DATABASE_URL=sqlite:///queries.db\n```\n\n### Step 3: Initialize System\n```bash\n# Install dependencies and setup\npython run.py install\n\n# Train initial models\npython run.py train\n\n# Start development server\npython run.py start\n```\n\n### Step 4: Verify Installation\n```bash\n# Test system functionality\npython scripts/test_system.py\n\n# Access services:\n# API: http://localhost:8000/docs\n# Health: http://localhost:8000/health\n# Metrics: http://localhost:8000/metrics\n```\n\n## Docker Deployment\n\n### Single Container\n```bash\n# Build and run API only\ndocker build -t customer-query-system .\ndocker run -p 8000:8000 customer-query-system\n```\n\n### Full Stack with Docker Compose\n```bash\n# Run complete system with monitoring\ndocker-compose up --build\n\n# Run in background\ndocker-compose up -d --build\n\n# View logs\ndocker-compose logs -f\n\n# Stop services\ndocker-compose down\n```\n\n### Services Available:\n- **API**: http://localhost:8000\n- **MLflow**: http://localhost:5000\n- **Prometheus**: http://localhost:9090\n\n## AWS Cloud Deployment\n\n### Prerequisites\n- AWS CLI configured\n- AWS account with appropriate permissions\n- Docker (for Lambda deployment)\n\n### Step 1: Deploy Infrastructure\n```bash\n# Deploy CloudFormation stack\naws cloudformation deploy \\\n  --template-file aws/cloudformation.yml \\\n  --stack-name customer-query-system \\\n  --capabilities CAPABILITY_IAM \\\n  --region us-east-1\n```\n\n### Step 2: Deploy Lambda Function\n```bash\n# Package Lambda function\nzip -r lambda-deployment.zip aws/lambda_function.py\n\n# Update Lambda function\naws lambda update-function-code \\\n  --function-name query-processor \\\n  --zip-file fileb://lambda-deployment.zip \\\n  --region us-east-1\n```\n\n### Step 3: Configure API Gateway\n```bash\n# Get API Gateway URL from CloudFormation outputs\naws cloudformation describe-stacks \\\n  --stack-name customer-query-system \\\n  --query 'Stacks[0].Outputs[?OutputKey==`ApiEndpoint`].OutputValue' \\\n  --output text\n```\n\n## Production Deployment\n\n### Kubernetes Deployment\n\n```yaml\n# k8s-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: customer-query-system\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: customer-query-system\n  template:\n    metadata:\n      labels:\n        app: customer-query-system\n    spec:\n      containers:\n      - name: api\n        image: customer-query-system:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: DATABASE_URL\n          value: \"postgresql://user:pass@postgres:5432/queries\"\n        - name: MLFLOW_TRACKING_URI\n          value: \"http://mlflow:5000\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: customer-query-service\nspec:\n  selector:\n    app: customer-query-system\n  ports:\n  - port: 80\n    targetPort: 8000\n  type: LoadBalancer\n```\n\n```bash\n# Deploy to Kubernetes\nkubectl apply -f k8s-deployment.yaml\n\n# Check deployment status\nkubectl get pods\nkubectl get services\n```\n\n### Environment Variables for Production\n\n```bash\n# Production .env\nAPI_HOST=0.0.0.0\nAPI_PORT=8000\nAPI_WORKERS=4\n\n# Database\nDATABASE_URL=postgresql://user:password@host:5432/queries\nREDIS_URL=redis://redis-host:6379\n\n# MLflow\nMLFLOW_TRACKING_URI=http://mlflow-server:5000\nMLFLOW_EXPERIMENT_NAME=customer-query-production\n\n# AWS\nAWS_REGION=us-east-1\nAWS_ACCESS_KEY_ID=your-access-key\nAWS_SECRET_ACCESS_KEY=your-secret-key\nS3_BUCKET=customer-query-models\n\n# Monitoring\nPROMETHEUS_PORT=9090\nLOG_LEVEL=INFO\nENABLE_METRICS=true\n```\n\n## Monitoring Setup\n\n### Prometheus Configuration\n```yaml\n# prometheus-production.yml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nscrape_configs:\n  - job_name: 'customer-query-api'\n    static_configs:\n      - targets: ['api:8000']\n    metrics_path: '/prometheus-metrics'\n    scrape_interval: 5s\n\n  - job_name: 'mlflow'\n    static_configs:\n      - targets: ['mlflow:5000']\n    scrape_interval: 30s\n```\n\n### Grafana Dashboard\n```bash\n# Add Grafana to docker-compose\nservices:\n  grafana:\n    image: grafana/grafana\n    ports:\n      - \"3000:3000\"\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=admin\n    volumes:\n      - grafana-data:/var/lib/grafana\n```\n\n## Performance Tuning\n\n### API Optimization\n```python\n# gunicorn configuration\n# gunicorn_config.py\nbind = \"0.0.0.0:8000\"\nworkers = 4\nworker_class = \"uvicorn.workers.UvicornWorker\"\nworker_connections = 1000\nmax_requests = 1000\nmax_requests_jitter = 100\npreload_app = True\nkeepalive = 5\n```\n\n### Database Optimization\n```sql\n-- Add indexes for better performance\nCREATE INDEX idx_customer_id ON queries(customer_id);\nCREATE INDEX idx_timestamp ON queries(timestamp);\nCREATE INDEX idx_intent ON queries(intent);\nCREATE INDEX idx_confidence ON queries(confidence);\n```\n\n### Caching Strategy\n```python\n# Redis caching configuration\nREDIS_CONFIG = {\n    'host': 'redis-host',\n    'port': 6379,\n    'db': 0,\n    'decode_responses': True,\n    'max_connections': 100\n}\n\n# Cache model predictions\n@cache(expire=3600)  # 1 hour cache\ndef get_model_prediction(query_hash):\n    return model.predict(query)\n```\n\n## Security Configuration\n\n### API Security\n```python\n# Add to main.py\nfrom fastapi.middleware.trustedhost import TrustedHostMiddleware\nfrom fastapi.middleware.httpsredirect import HTTPSRedirectMiddleware\n\n# HTTPS redirect\napp.add_middleware(HTTPSRedirectMiddleware)\n\n# Trusted hosts\napp.add_middleware(\n    TrustedHostMiddleware, \n    allowed_hosts=[\"yourdomain.com\", \"*.yourdomain.com\"]\n)\n\n# Rate limiting\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.util import get_remote_address\nfrom slowapi.errors import RateLimitExceeded\n\nlimiter = Limiter(key_func=get_remote_address)\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\n@app.post(\"/analyze-query\")\n@limiter.limit(\"100/minute\")\nasync def analyze_query(request: Request, query_request: QueryRequest):\n    # ... existing code\n```\n\n### Environment Security\n```bash\n# Use secrets management\n# AWS Secrets Manager\naws secretsmanager create-secret \\\n  --name customer-query-system/database \\\n  --secret-string '{\"username\":\"dbuser\",\"password\":\"securepassword\"}'\n\n# Kubernetes secrets\nkubectl create secret generic db-credentials \\\n  --from-literal=username=dbuser \\\n  --from-literal=password=securepassword\n```\n\n## Backup and Recovery\n\n### Database Backup\n```bash\n# SQLite backup\ncp queries.db queries_backup_$(date +%Y%m%d_%H%M%S).db\n\n# PostgreSQL backup\npg_dump -h localhost -U username -d queries > backup_$(date +%Y%m%d_%H%M%S).sql\n```\n\n### Model Backup\n```bash\n# Backup MLflow artifacts\naws s3 sync mlruns/ s3://your-backup-bucket/mlruns/\n\n# Backup model files\ntar -czf models_backup_$(date +%Y%m%d_%H%M%S).tar.gz models/\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Port Already in Use**\n```bash\n# Find process using port 8000\nlsof -i :8000\n# Kill process\nkill -9 <PID>\n```\n\n2. **Memory Issues**\n```bash\n# Monitor memory usage\ndocker stats\n# Increase Docker memory limit\n# Docker Desktop > Settings > Resources > Memory\n```\n\n3. **Model Loading Errors**\n```bash\n# Clear model cache\nrm -rf ~/.cache/huggingface/\n# Reinstall transformers\npip uninstall transformers\npip install transformers\n```\n\n4. **Database Connection Issues**\n```bash\n# Check database file permissions\nls -la queries.db\n# Reset database\nrm queries.db\npython run.py install\n```\n\n### Logs and Debugging\n\n```bash\n# View application logs\ndocker-compose logs -f api\n\n# Enable debug mode\nexport LOG_LEVEL=DEBUG\npython run.py start\n\n# Check system resources\ndocker system df\ndocker system prune\n```\n\n## Health Checks\n\n### Automated Health Monitoring\n```bash\n#!/bin/bash\n# health_check.sh\n\nAPI_URL=\"http://localhost:8000\"\n\n# Check API health\nif curl -f \"$API_URL/health\" > /dev/null 2>&1; then\n    echo \"‚úÖ API is healthy\"\nelse\n    echo \"‚ùå API is down\"\n    exit 1\nfi\n\n# Check model performance\nCONFIDENCE=$(curl -s \"$API_URL/metrics\" | jq '.avg_confidence')\nif (( $(echo \"$CONFIDENCE > 0.7\" | bc -l) )); then\n    echo \"‚úÖ Model performance is good\"\nelse\n    echo \"‚ö†Ô∏è Model performance is degraded\"\nfi\n```\n\n### Monitoring Alerts\n```yaml\n# alertmanager.yml\nroute:\n  group_by: ['alertname']\n  group_wait: 10s\n  group_interval: 10s\n  repeat_interval: 1h\n  receiver: 'web.hook'\n\nreceivers:\n- name: 'web.hook'\n  webhook_configs:\n  - url: 'http://localhost:5001/'\n\ninhibit_rules:\n  - source_match:\n      severity: 'critical'\n    target_match:\n      severity: 'warning'\n    equal: ['alertname', 'dev', 'instance']\n```\n\nThis deployment guide provides comprehensive instructions for deploying the AI Customer Query System in various environments, from local development to production cloud deployments.